"""
Validation Extension with Interactive Chat Interface
Cho ph√©p ng∆∞·ªùi d√πng chat v·ªõi agent ngay trong tab Questions v√† Scenarios
H·ªñ TR·ª¢ QUERY NHI·ªÄU B·∫¢NG (Multi-Table) V√Ä STREAMING
"""

import streamlit as st
import pandas as pd
from typing import Dict, Any, List, Optional, Generator
from datetime import datetime
from pathlib import Path
import sqlite3
import re
import logging
import json

logger = logging.getLogger(__name__)


# ============================================================================
# SQL Query Tool (Single Table - GI·ªÆ NGUY√äN ƒê·ªÇ BACKWARD COMPATIBLE)
# ============================================================================

class SQLQueryTool:
    """Tool to execute SQL queries - thread-safe for Streamlit"""

    def __init__(self, db_path: str, df: pd.DataFrame, table_name: str = "data"):
        self.db_path = db_path
        self.table_name = table_name
        self.df_hash = hash(str(df.values.tobytes()))
        self._create_database(df)

    def _create_database(self, df: pd.DataFrame):
        """Create/recreate database with data"""
        try:
            if hasattr(self, 'conn') and self.conn:
                self.conn.close()
            self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
            df.to_sql(self.table_name, self.conn, if_exists='replace', index=False)
        except Exception as e:
            raise RuntimeError(f"Failed to create database: {str(e)}")

    def _ensure_connection(self):
        """Ensure connection is valid, recreate if needed"""
        try:
            if not self.conn:
                self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
            self.conn.execute("SELECT 1")
        except:
            self.conn = sqlite3.connect(self.db_path, check_same_thread=False)

    def execute_query(self, query: str):
        try:
            if not query.upper().strip().startswith('SELECT'):
                return pd.DataFrame(), "Only SELECT allowed"
            self._ensure_connection()
            result_df = pd.read_sql_query(query, self.conn)
            return result_df, None
        except Exception as e:
            return pd.DataFrame(), f"Error: {str(e)}"

    def get_schema_info(self):
        try:
            self._ensure_connection()
            cursor = self.conn.cursor()
            cursor.execute(f"PRAGMA table_info({self.table_name})")
            columns = cursor.fetchall()
            cursor.execute(f"SELECT COUNT(*) FROM {self.table_name}")
            row_count = cursor.fetchone()[0]
            return {
                "table_name": self.table_name,
                "columns": [{"name": col[1], "type": col[2]} for col in columns],
                "row_count": row_count
            }
        except Exception as e:
            return {"error": str(e)}

    def close(self):
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()


# ============================================================================
# Multi-Table SQL Query Tool - H·ªñ TR·ª¢ QUERY NHI·ªÄU B·∫¢NG
# ============================================================================

class MultiTableSQLQueryTool:
    """
    SQL Query Tool h·ªó tr·ª£ nhi·ªÅu b·∫£ng.
    Cho ph√©p ng∆∞·ªùi d√πng query tr√™n nhi·ªÅu DataFrame/b·∫£ng c√πng l√∫c.
    """
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.tables: Dict[str, Dict[str, Any]] = {}
        self.conn = None
        self._create_connection()
    
    def _create_connection(self):
        try:
            if self.conn:
                self.conn.close()
            self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        except Exception as e:
            raise RuntimeError(f"Failed to create database connection: {str(e)}")
    
    def _ensure_connection(self):
        try:
            if not self.conn:
                self._create_connection()
            self.conn.execute("SELECT 1")
        except:
            self._create_connection()
    
    def _sanitize_table_name(self, name: str) -> str:
        """Sanitize table name for SQL"""
        safe_name = re.sub(r'[^\w]', '_', str(name))
        if safe_name and safe_name[0].isdigit():
            safe_name = 't_' + safe_name
        return safe_name or 'data'
    
    def add_table(self, table_name: str, df: pd.DataFrame, schema_info: Dict = None) -> bool:
        """Th√™m ho·∫∑c c·∫≠p nh·∫≠t m·ªôt b·∫£ng v√†o database"""
        try:
            safe_name = self._sanitize_table_name(table_name)
            df_hash = hash(str(df.values.tobytes()))
            
            self._ensure_connection()
            df.to_sql(safe_name, self.conn, if_exists='replace', index=False)
            
            self.tables[safe_name] = {
                "df_hash": df_hash,
                "columns": list(df.columns),
                "row_count": len(df),
                "original_name": table_name,
                "schema_info": schema_info or {}
            }
            return True
        except Exception as e:
            logger.error(f"Failed to add table '{table_name}': {str(e)}")
            return False
    
    def remove_table(self, table_name: str) -> bool:
        """X√≥a m·ªôt b·∫£ng kh·ªèi database"""
        try:
            safe_name = self._sanitize_table_name(table_name)
            if safe_name not in self.tables:
                return False
            self._ensure_connection()
            self.conn.execute(f"DROP TABLE IF EXISTS [{safe_name}]")
            del self.tables[safe_name]
            return True
        except Exception as e:
            return False
    
    def execute_query(self, query: str) -> tuple:
        """Th·ª±c thi SQL query"""
        try:
            if not query.upper().strip().startswith('SELECT'):
                return pd.DataFrame(), "Ch·ªâ cho ph√©p SELECT queries"
            self._ensure_connection()
            result_df = pd.read_sql_query(query, self.conn)
            return result_df, None
        except Exception as e:
            return pd.DataFrame(), f"SQL Error: {str(e)}"
    
    def get_tables_info(self) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin t·∫•t c·∫£ c√°c b·∫£ng"""
        return {
            "tables": [
                {
                    "name": name,
                    "original_name": info.get("original_name", name),
                    "columns": info.get("columns", []),
                    "row_count": info.get("row_count", 0),
                }
                for name, info in self.tables.items()
            ],
            "total_tables": len(self.tables)
        }
    
    def get_schema_info(self) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin schema cho t·∫•t c·∫£ c√°c b·∫£ng (t∆∞∆°ng th√≠ch v·ªõi SQLQueryTool)"""
        return self.get_tables_info()
    
    def close(self):
        if self.conn:
            try:
                self.conn.close()
            except:
                pass
            self.conn = None


# ============================================================================
# Setup Helper Functions
# ============================================================================

def setup_sql_tool(session, df_cleaned):
    """Setup or update SQL tool for chat (Single table - backward compatible)"""
    current_df_hash = hash(str(df_cleaned.values.tobytes()))
    
    need_recreate = False
    if 'sql_tool' not in st.session_state or st.session_state.sql_tool is None:
        need_recreate = True
    elif 'sql_tool_df_hash' not in st.session_state:
        need_recreate = True
    elif st.session_state.sql_tool_df_hash != current_df_hash:
        need_recreate = True
    
    if need_recreate:
        db_dir = Path("./agent_databases")
        db_dir.mkdir(exist_ok=True)
        db_path = db_dir / f"data_{session.session_id}.db"
        
        if 'sql_tool' in st.session_state and st.session_state.sql_tool:
            try:
                st.session_state.sql_tool.close()
            except:
                pass
        
        sql_tool = SQLQueryTool(str(db_path), df_cleaned)
        st.session_state.sql_tool = sql_tool
        st.session_state.sql_tool_df_hash = current_df_hash
        
        return sql_tool, True
    
    return st.session_state.sql_tool, False


def setup_multi_table_sql_tool(session, sources_dfs: Dict[str, pd.DataFrame], 
                                schemas: Dict[str, Dict] = None) -> MultiTableSQLQueryTool:
    """
    Setup ho·∫∑c c·∫≠p nh·∫≠t multi-table SQL tool.
    """
    combined_hash = hash(tuple(
        hash(str(df.values.tobytes())) 
        for df in sources_dfs.values()
    ))
    
    need_recreate = False
    if 'multi_sql_tool' not in st.session_state or st.session_state.multi_sql_tool is None:
        need_recreate = True
    elif 'multi_sql_tool_hash' not in st.session_state:
        need_recreate = True
    elif st.session_state.multi_sql_tool_hash != combined_hash:
        need_recreate = True
    
    if need_recreate:
        db_dir = Path("./agent_databases")
        db_dir.mkdir(exist_ok=True)
        db_path = db_dir / f"multi_data_{session.session_id}.db"
        
        if 'multi_sql_tool' in st.session_state and st.session_state.multi_sql_tool:
            try:
                st.session_state.multi_sql_tool.close()
            except:
                pass
        
        sql_tool = MultiTableSQLQueryTool(str(db_path))
        
        for source_id, df in sources_dfs.items():
            schema_info = {}
            if schemas and source_id in schemas:
                schema_info = {
                    col: {
                        "description": col_schema.description if hasattr(col_schema, 'description') else "",
                        "semantic_type": col_schema.semantic_type if hasattr(col_schema, 'semantic_type') else "",
                    }
                    for col, col_schema in schemas[source_id].items()
                }
            sql_tool.add_table(source_id, df, schema_info)
        
        st.session_state.multi_sql_tool = sql_tool
        st.session_state.multi_sql_tool_hash = combined_hash
        
        return sql_tool
    
    return st.session_state.multi_sql_tool


def get_all_available_dataframes(session) -> Dict[str, pd.DataFrame]:
    """L·∫•y t·∫•t c·∫£ c√°c DataFrame c√≥ s·∫µn t·ª´ session state."""
    sources_dfs = {}
    
    for source in session.sources:
        source_id = source.source_id
        df = st.session_state.cleaned_dfs.get(
            source_id,
            st.session_state.clean_dfs.get(
                source_id,
                st.session_state.raw_dfs.get(source_id)
            )
        )
        if df is not None:
            sources_dfs[source_id] = df
    
    return sources_dfs


# ============================================================================
# Chat State Management
# ============================================================================

def initialize_chat_state(context_key: str):
    chat_key = f"chat_history_{context_key}"
    if chat_key not in st.session_state:
        st.session_state[chat_key] = []


def get_chat_history(context_key: str) -> List[Dict[str, str]]:
    chat_key = f"chat_history_{context_key}"
    return st.session_state.get(chat_key, [])


def add_to_chat_history(context_key: str, role: str, content: str):
    chat_key = f"chat_history_{context_key}"
    if chat_key not in st.session_state:
        st.session_state[chat_key] = []
    
    st.session_state[chat_key].append({
        "role": role,
        "content": content,
        "timestamp": datetime.now().isoformat()
    })


def clear_chat_history(context_key: str):
    chat_key = f"chat_history_{context_key}"
    st.session_state[chat_key] = []


# ============================================================================
# Chat Interface Components
# ============================================================================

def render_chat_message(role: str, content: str, timestamp: str = None):
    if role == "user":
        st.markdown(f"""
        <div class="chat-message user-message">
            <strong>B·∫°n:</strong><br>{content}
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="chat-message assistant-message">
            <strong>Agent:</strong><br>{content}
        </div>
        """, unsafe_allow_html=True)


def stream_agent_response(agent, question: str, placeholder) -> Generator[str, None, None]:
    """
    Stream response t·ª´ agent (Single Table) d∆∞·ªõi d·∫°ng Generator.
    Gi√∫p UI hi·ªÉn th·ªã m∆∞·ª£t m√† t·ª´ng t·ª´.
    """
    full_response = ""
    try:
        for chunk in agent.query(question):
            if isinstance(chunk, dict):
                continue
            full_response += chunk
            placeholder.markdown(full_response + "‚ñå")
            yield chunk
        
        # K·∫øt th√∫c stream, hi·ªÉn th·ªã b·∫£n clean (b·ªè con tr·ªè)
        placeholder.markdown(full_response)
        
    except Exception as e:
        error_msg = f"‚ùå L·ªói: {str(e)}"
        placeholder.markdown(error_msg)
        yield error_msg


# ============================================================================
# DEBUG MODE FUNCTIONS - Show LLM Reasoning Process
# ============================================================================

def stream_agent_response_debug(agent, question: str, placeholder) -> Generator[str, None, None]:
    """
    DEBUG MODE: Stream response t·ª´ agent v·ªõi debug info
    Hi·ªÉn th·ªã c√°ch LLM suy nghƒ© v√† query
    """
    from openai import OpenAI
    import json

    full_response = ""
    debug_info = []

    try:
        # Hi·ªÉn th·ªã debug header
        debug_header = "### üîç DEBUG MODE - LLM Query Process\n\n"
        placeholder.markdown(debug_header)
        full_response += debug_header
        yield debug_header

        # 1. Hi·ªÉn th·ªã c√¢u h·ªèi
        q_section = f"**üìù C√¢u h·ªèi:** {question}\n\n"
        full_response += q_section
        placeholder.markdown(full_response + "‚ñå")
        yield q_section

        # 2. Hi·ªÉn th·ªã context/schema
        context_section = "**üìä Context ƒë∆∞·ª£c g·ª≠i cho LLM:**\n"
        if agent.session.schema:
            context_section += "```\n"
            for col_name, col_schema in list(agent.session.schema.items())[:5]:
                if hasattr(col_schema, 'semantic_type'):
                    context_section += f"- {col_name}: {col_schema.semantic_type}\n"
            context_section += "```\n\n"

        full_response += context_section
        placeholder.markdown(full_response + "‚ñå")
        yield context_section

        # 3. G·ªçi agent v√† capture response
        reasoning_section = "**üß† LLM Reasoning:**\n"
        full_response += reasoning_section
        placeholder.markdown(full_response + "‚ñå")
        yield reasoning_section

        agent_response = ""
        for chunk in agent.query(question):
            if isinstance(chunk, dict):
                # C√≥ th·ªÉ l√† metadata ho·∫∑c SQL query info
                if 'sql_query' in chunk:
                    sql_section = f"\n\n**‚ö° SQL Query ƒë∆∞·ª£c t·∫°o:**\n```sql\n{chunk['sql_query']}\n```\n\n"
                    full_response += sql_section
                    placeholder.markdown(full_response + "‚ñå")
                    yield sql_section
                continue
            agent_response += chunk
            full_response += chunk
            placeholder.markdown(full_response + "‚ñå")
            yield chunk

        # 4. Final answer section
        final_section = f"\n\n---\n### ‚úÖ C√¢u tr·∫£ l·ªùi cu·ªëi c√πng:\n{agent_response}\n"
        full_response = full_response.replace(agent_response, "")  # Remove duplicate
        full_response += final_section
        placeholder.markdown(full_response)
        yield "\n" + final_section

    except Exception as e:
        error_msg = f"\n\n‚ùå **L·ªói:** {str(e)}\n"
        full_response += error_msg
        placeholder.markdown(full_response)
        yield error_msg


def _stream_multi_table_query_debug(question: str, sql_tool: MultiTableSQLQueryTool,
                                     session, placeholder) -> Generator[str, None, None]:
    """
    DEBUG MODE: Query agent v·ªõi multi-table context V·ªöI DEBUG INFO
    Hi·ªÉn th·ªã to√†n b·ªô qu√° tr√¨nh LLM suy nghƒ©, query SQL, v√† tr·∫£ l·ªùi
    """
    from openai import OpenAI
    import json

    client = OpenAI(api_key=st.session_state.api_key)
    context = _build_multi_table_context_debug(sql_tool, session)

    system_prompt = """B·∫°n l√† m·ªôt data analyst th√¥ng minh v·ªõi kh·∫£ nƒÉng query SQL tr√™n NHI·ªÄU B·∫¢NG.

**NGUY√äN T·∫ÆC:**
1. D√πng [table_name] cho t√™n b·∫£ng
2. D√πng table.column khi c·∫ßn ph√¢n bi·ªát
3. C√≥ th·ªÉ JOIN, UNION nhi·ªÅu b·∫£ng
4. LU√îN tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT
5. N·∫øu c√≥ business rules ‚Üí tu√¢n theo

Khi c·∫ßn d·ªØ li·ªáu, d√πng tool execute_sql_query."""

    full_response = ""

    try:
        # === DEBUG SECTION 1: Hi·ªÉn th·ªã System Prompt ===
        debug_header = "### üîç DEBUG MODE - LLM Query Process\n\n"
        full_response += debug_header
        placeholder.markdown(full_response)
        yield debug_header

        prompt_section = f"**üìù System Prompt:**\n```\n{system_prompt[:200]}...\n```\n\n"
        full_response += prompt_section
        placeholder.markdown(full_response + "‚ñå")
        yield prompt_section

        # === DEBUG SECTION 2: Hi·ªÉn th·ªã Context ===
        context_section = f"**üìä Context g·ª≠i cho LLM:**\n```\n{context[:500]}...\n```\n\n"
        full_response += context_section
        placeholder.markdown(full_response + "‚ñå")
        yield context_section

        # === DEBUG SECTION 3: Hi·ªÉn th·ªã Question ===
        q_section = f"**‚ùì C√¢u h·ªèi:** {question}\n\n"
        full_response += q_section
        placeholder.markdown(full_response + "‚ñå")
        yield q_section

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"**Context:**\n{context}\n\n**C√¢u h·ªèi:**\n{question}"}
        ]

        tools = [{
            "type": "function",
            "function": {
                "name": "execute_sql_query",
                "description": f"Th·ª±c thi SQL SELECT query tr√™n c√°c b·∫£ng: {', '.join(sql_tool.tables.keys())}. D√πng [table_name] cho t√™n b·∫£ng.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "SQL SELECT query"}
                    },
                    "required": ["query"]
                }
            }
        }]

        # === B∆Ø·ªöC 1: LLM Thinking ===
        thinking_section = "**üß† LLM ƒëang ph√¢n t√≠ch...**\n\n"
        full_response += thinking_section
        placeholder.markdown(full_response + "‚ñå")
        yield thinking_section

        response = client.chat.completions.create(
            model=st.session_state.get('model', 'gpt-4o-mini'),
            messages=messages,
            tools=tools,
            tool_choice="auto",
            temperature=0.7,
            max_tokens=2000
        )

        assistant_msg = response.choices[0].message

        # === B∆Ø·ªöC 2: Tool Call (SQL Execution) ===
        if assistant_msg.tool_calls:
            tool_section = "**‚ö° LLM quy·∫øt ƒë·ªãnh g·ªçi SQL tool:**\n\n"
            full_response += tool_section
            placeholder.markdown(full_response + "‚ñå")
            yield tool_section

            sql_results = []

            for tool_call in assistant_msg.tool_calls:
                if tool_call.function.name == "execute_sql_query":
                    args = json.loads(tool_call.function.arguments)
                    query = args.get("query", "")

                    # === DEBUG: Hi·ªÉn th·ªã SQL Query ===
                    sql_display = f"```sql\n{query}\n```\n\n"
                    full_response += sql_display
                    placeholder.markdown(full_response + "‚ñå")
                    yield sql_display

                    exec_msg = "**‚è≥ ƒêang th·ª±c thi SQL...**\n\n"
                    full_response += exec_msg
                    placeholder.markdown(full_response + "‚ñå")
                    yield exec_msg

                    result_df, error = sql_tool.execute_query(query)

                    if error:
                        error_result = f"‚ùå **SQL Error:** {error}\n\n"
                        full_response += error_result
                        sql_results.append(f"‚ùå SQL Error: {error}")
                        placeholder.markdown(full_response + "‚ñå")
                        yield error_result
                    else:
                        if len(result_df) == 0:
                            no_result = "‚ö†Ô∏è Query kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£\n\n"
                            full_response += no_result
                            sql_results.append("Query kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
                            placeholder.markdown(full_response + "‚ñå")
                            yield no_result
                        else:
                            # === DEBUG: Hi·ªÉn th·ªã SQL Results ===
                            result_display = f"**üìä K·∫øt qu·∫£ SQL ({len(result_df)} d√≤ng):**\n```\n{result_df.head(10).to_string(index=False)}\n```\n"
                            if len(result_df) > 10:
                                result_display += f"*(Hi·ªÉn th·ªã 10/{len(result_df)} d√≤ng)*\n\n"

                            full_response += result_display
                            placeholder.markdown(full_response + "‚ñå")
                            yield result_display

                            result_text = f"K·∫øt qu·∫£ ({len(result_df)} d√≤ng):\n```\n{result_df.head(15).to_string(index=False)}\n```"
                            if len(result_df) > 15:
                                result_text += f"\n(Hi·ªÉn th·ªã 15/{len(result_df)} d√≤ng)"
                            sql_results.append(result_text)

            result_summary = "\n\n".join(sql_results)

            # === B∆Ø·ªöC 3: Final Analysis ===
            analysis_header = "**üìù LLM ƒëang ph√¢n t√≠ch k·∫øt qu·∫£ ƒë·ªÉ tr·∫£ l·ªùi...**\n\n"
            full_response += analysis_header
            placeholder.markdown(full_response + "‚ñå")
            yield analysis_header

            final_stream = client.chat.completions.create(
                model=st.session_state.get('model', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": "Ph√¢n t√≠ch k·∫øt qu·∫£ SQL v√† tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát. Tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† r√µ r√†ng. Format markdown ƒë·∫πp."},
                    {"role": "user", "content": f"C√¢u h·ªèi: {question}\n\nK·∫øt qu·∫£ SQL:\n{result_summary}\n\nPh√¢n t√≠ch v√† tr·∫£ l·ªùi:"}
                ],
                temperature=0.7,
                max_tokens=1500,
                stream=True
            )

            final_answer = ""
            for chunk in final_stream:
                if chunk.choices[0].delta.content:
                    text = chunk.choices[0].delta.content
                    final_answer += text
                    full_response += text
                    placeholder.markdown(full_response + "‚ñå")
                    yield text

            # === FINAL SECTION ===
            final_section = f"\n\n---\n### ‚úÖ C√¢u tr·∫£ l·ªùi cu·ªëi c√πng:\n{final_answer}\n"
            placeholder.markdown(full_response + final_section)
            yield f"\n{final_section}"

        else:
            # Tr∆∞·ªùng h·ª£p kh√¥ng g·ªçi tool
            if assistant_msg.content:
                direct_answer = f"**üí° LLM tr·∫£ l·ªùi tr·ª±c ti·∫øp (kh√¥ng c·∫ßn SQL):**\n\n{assistant_msg.content}\n"
                full_response += direct_answer
                placeholder.markdown(full_response)
                yield direct_answer
            else:
                no_answer = "‚ö†Ô∏è Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi t·ª´ Agent.\n"
                full_response += no_answer
                placeholder.markdown(full_response)
                yield no_answer

    except Exception as e:
        error_msg = f"\n\n‚ùå **L·ªói:** {str(e)}\n"
        full_response += error_msg
        placeholder.markdown(full_response)
        yield error_msg


def _build_multi_table_context_debug(sql_tool: MultiTableSQLQueryTool, session) -> str:
    """Build context cho debug mode - KH√îNG bao g·ªìm additional_notes v√† scenarios"""
    context_parts = []

    context_parts.append("**üìä C√ÅC B·∫¢NG D·ªÆ LI·ªÜU:**")
    for table_name, info in sql_tool.tables.items():
        cols_preview = ', '.join(str(c) for c in info['columns'][:10])
        if len(info['columns']) > 10:
            cols_preview += f", ... (+{len(info['columns']) - 10} c·ªôt)"
        context_parts.append(f"- **{table_name}** ({info['row_count']} rows): {cols_preview}")

    if session.schema:
        context_parts.append("\n**üìã CHI TI·∫æT SCHEMA:**")
        for col_name, col_schema in list(session.schema.items()):
            if hasattr(col_schema, 'semantic_type') and hasattr(col_schema, 'description'):
                desc = col_schema.description[:60] if col_schema.description else ''
                context_parts.append(f"- {col_name}: {col_schema.semantic_type} - {desc}")

    # KH√îNG th√™m additional_notes v√† scenarios trong debug mode
    context_parts.append("\n‚ö†Ô∏è **NOTE:** Debug mode - Additional notes v√† scenarios CH∆ØA ƒë∆∞·ª£c th√™m v√†o context n√†y.")

    return "\n".join(context_parts)


# ============================================================================
# Multi-Table Query with Streaming Support
# ============================================================================

def _build_multi_table_context(sql_tool: MultiTableSQLQueryTool, session) -> str:
    """Build context string for multi-table query"""
    context_parts = []
    
    context_parts.append("**üìä C√ÅC B·∫¢NG D·ªÆ LI·ªÜU:**")
    for table_name, info in sql_tool.tables.items():
        cols_preview = ', '.join(str(c) for c in info['columns'][:10])
        if len(info['columns']) > 10:
            cols_preview += f", ... (+{len(info['columns']) - 10} c·ªôt)"
        context_parts.append(f"- **{table_name}** ({info['row_count']} rows): {cols_preview}")
    
    if session.schema:
        context_parts.append("\n**üìã CHI TI·∫æT SCHEMA:**")
        for col_name, col_schema in list(session.schema.items()):
            if hasattr(col_schema, 'semantic_type') and hasattr(col_schema, 'description'):
                desc = col_schema.description[:60] if col_schema.description else ''
                context_parts.append(f"- {col_name}: {col_schema.semantic_type} - {desc}")
    
    if session.question_set and session.question_set.additional_notes:
        context_parts.append(f"\n**‚ö†Ô∏è QUY T·∫ÆC NGHI·ªÜP V·ª§:**\n{session.question_set.additional_notes[:500]}")
    
    if session.scenarios:
        context_parts.append("\n**üéØ SCENARIOS:**")
        for sc in session.scenarios:
            context_parts.append(f"- {sc.name}: {sc.description if sc.description else 'N/A'}")
    
    return "\n".join(context_parts)


def _stream_multi_table_query(question: str, sql_tool: MultiTableSQLQueryTool, 
                               session, placeholder) -> Generator[str, None, None]:
    """
    Query agent v·ªõi multi-table context V·ªöI STREAMING.
    H√†m n√†y yield t·ª´ng chunk text ƒë·ªÉ UI c·∫≠p nh·∫≠t.
    """
    from openai import OpenAI
    
    client = OpenAI(api_key=st.session_state.api_key)
    context = _build_multi_table_context(sql_tool, session)
    
    system_prompt = """B·∫°n l√† m·ªôt data analyst th√¥ng minh v·ªõi kh·∫£ nƒÉng query SQL tr√™n NHI·ªÄU B·∫¢NG.

**NGUY√äN T·∫ÆC:**
1. D√πng [table_name] cho t√™n b·∫£ng
2. D√πng table.column khi c·∫ßn ph√¢n bi·ªát
3. C√≥ th·ªÉ JOIN, UNION nhi·ªÅu b·∫£ng
4. LU√îN tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT
5. N·∫øu c√≥ business rules ‚Üí tu√¢n theo

Khi c·∫ßn d·ªØ li·ªáu, d√πng tool execute_sql_query."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"**Context:**\n{context}\n\n**C√¢u h·ªèi:**\n{question}"}
    ]
    
    tools = [{
        "type": "function",
        "function": {
            "name": "execute_sql_query",
            "description": f"Th·ª±c thi SQL SELECT query tr√™n c√°c b·∫£ng: {', '.join(sql_tool.tables.keys())}. D√πng [table_name] cho t√™n b·∫£ng.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "SQL SELECT query"}
                },
                "required": ["query"]
            }
        }
    }]
    
    full_response = ""
    
    try:
        # B∆∞·ªõc 1: Agent suy nghƒ© v√† g·ªçi Tool (kh√¥ng stream ph·∫ßn n√†y, ch·ªâ hi·ªán status)
        placeholder.markdown("üîç Agent ƒëang ph√¢n t√≠ch...")
        
        response = client.chat.completions.create(
            model=st.session_state.get('model', 'gpt-4o-mini'),
            messages=messages,
            tools=tools,
            tool_choice="auto",
            temperature=0.7,
            max_tokens=2000
        )
        
        assistant_msg = response.choices[0].message
        
        # B∆∞·ªõc 2: X·ª≠ l√Ω Tool Call (SQL)
        if assistant_msg.tool_calls:
            sql_results = []
            
            for tool_call in assistant_msg.tool_calls:
                if tool_call.function.name == "execute_sql_query":
                    args = json.loads(tool_call.function.arguments)
                    query = args.get("query", "")
                    
                    placeholder.markdown(f"üîç ƒêang th·ª±c thi SQL:\n```sql\n{query}\n```")
                    
                    result_df, error = sql_tool.execute_query(query)
                    
                    if error:
                        sql_results.append(f"‚ùå SQL Error: {error}")
                    else:
                        if len(result_df) == 0:
                            sql_results.append("Query kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
                        else:
                            # Format k·∫øt qu·∫£ g·ªçn g√†ng
                            result_text = f"K·∫øt qu·∫£ ({len(result_df)} d√≤ng):\n```\n{result_df.head(15).to_string(index=False)}\n```"
                            if len(result_df) > 15:
                                result_text += f"\n(Hi·ªÉn th·ªã 15/{len(result_df)} d√≤ng)"
                            sql_results.append(result_text)
            
            result_summary = "\n\n".join(sql_results)
            
            # B∆∞·ªõc 3: Agent ph√¢n t√≠ch k·∫øt qu·∫£ cu·ªëi c√πng (STREAMING)
            placeholder.markdown("üìù ƒêang ph√¢n t√≠ch k·∫øt qu·∫£...")
            
            final_stream = client.chat.completions.create(
                model=st.session_state.get('model', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": "Ph√¢n t√≠ch k·∫øt qu·∫£ SQL v√† tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát. Tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† r√µ r√†ng. Format markdown ƒë·∫πp."},
                    {"role": "user", "content": f"C√¢u h·ªèi: {question}\n\nK·∫øt qu·∫£ SQL:\n{result_summary}\n\nPh√¢n t√≠ch v√† tr·∫£ l·ªùi:"}
                ],
                temperature=0.7,
                max_tokens=1500,
                stream=True
            )
            
            for chunk in final_stream:
                if chunk.choices[0].delta.content:
                    text = chunk.choices[0].delta.content
                    full_response += text
                    # C·∫≠p nh·∫≠t UI ngay l·∫≠p t·ª©c
                    placeholder.markdown(full_response + "‚ñå")
                    # Yield text ƒë·ªÉ caller c√≥ th·ªÉ x·ª≠ l√Ω n·∫øu c·∫ßn
                    yield text

            placeholder.markdown(full_response)
            
        else:
            # Tr∆∞·ªùng h·ª£p kh√¥ng g·ªçi tool, tr·∫£ l·ªùi tr·ª±c ti·∫øp
            if assistant_msg.content:
                full_response = assistant_msg.content
                placeholder.markdown(full_response)
                yield full_response
            else:
                full_response = "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi t·ª´ Agent."
                placeholder.markdown(full_response)
                yield full_response
            
    except Exception as e:
        error_msg = f"‚ùå L·ªói: {str(e)}"
        placeholder.markdown(error_msg)
        yield error_msg

def _query_multi_table_for_chat(question: str, sql_tool: MultiTableSQLQueryTool, session) -> str:
    """Query agent v·ªõi multi-table context (KH√îNG STREAMING - backward compatible)"""
    from openai import OpenAI
    
    client = OpenAI(api_key=st.session_state.api_key)
    context = _build_multi_table_context(sql_tool, session)
    
    system_prompt = """B·∫°n l√† m·ªôt data analyst th√¥ng minh v·ªõi kh·∫£ nƒÉng query SQL tr√™n NHI·ªÄU B·∫¢NG.

**NGUY√äN T·∫ÆC:**
1. D√πng [table_name] cho t√™n b·∫£ng
2. D√πng table.column khi c·∫ßn ph√¢n bi·ªát
3. C√≥ th·ªÉ JOIN, UNION nhi·ªÅu b·∫£ng
4. LU√îN tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT
5. N·∫øu c√≥ business rules ‚Üí tu√¢n theo

Khi c·∫ßn d·ªØ li·ªáu, d√πng tool execute_sql_query."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"**Context:**\n{context}\n\n**C√¢u h·ªèi:**\n{question}"}
    ]
    
    tools = [{
        "type": "function",
        "function": {
            "name": "execute_sql_query",
            "description": f"Th·ª±c thi SQL SELECT query tr√™n c√°c b·∫£ng: {', '.join(sql_tool.tables.keys())}. D√πng [table_name] cho t√™n b·∫£ng.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "SQL SELECT query"}
                },
                "required": ["query"]
            }
        }
    }]
    
    try:
        response = client.chat.completions.create(
            model=st.session_state.get('model', 'gpt-4o-mini'),
            messages=messages,
            tools=tools,
            tool_choice="auto",
            temperature=0.7,
            max_tokens=2000
        )
        
        assistant_msg = response.choices[0].message
        
        if assistant_msg.tool_calls:
            sql_results = []
            
            for tool_call in assistant_msg.tool_calls:
                if tool_call.function.name == "execute_sql_query":
                    args = json.loads(tool_call.function.arguments)
                    query = args.get("query", "")
                    
                    result_df, error = sql_tool.execute_query(query)
                    
                    if error:
                        sql_results.append(f"‚ùå SQL Error: {error}")
                    else:
                        if len(result_df) == 0:
                            sql_results.append("Query kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
                        else:
                            result_text = f"K·∫øt qu·∫£ ({len(result_df)} d√≤ng):\n```\n{result_df.head(15).to_string(index=False)}\n```"
                            if len(result_df) > 15:
                                result_text += f"\n(Hi·ªÉn th·ªã 15/{len(result_df)} d√≤ng)"
                            sql_results.append(result_text)
            
            result_summary = "\n\n".join(sql_results)
            
            final_response = client.chat.completions.create(
                model=st.session_state.get('model', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": "Ph√¢n t√≠ch k·∫øt qu·∫£ SQL v√† tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát."},
                    {"role": "user", "content": f"C√¢u h·ªèi: {question}\n\nK·∫øt qu·∫£ SQL:\n{result_summary}\n\nPh√¢n t√≠ch v√† tr·∫£ l·ªùi:"}
                ],
                temperature=0.7,
                max_tokens=1500
            )
            
            return final_response.choices[0].message.content
        else:
            return assistant_msg.content or "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi"
            
    except Exception as e:
        return f"‚ùå L·ªói: {str(e)}"



def render_chat_interface(
    session,
    agent,
    context_key: str,
    placeholder_text: str = "H·ªèi m·ªôt c√¢u ƒë·ªÉ test...",
    quick_questions: List[str] = None
):
    """Render chat interface for validation with STREAMING support (Generator Safe)"""
    initialize_chat_state(context_key)
    
    chat_history = get_chat_history(context_key)
    
    # Hi·ªÉn th·ªã l·ªãch s·ª≠
    if not chat_history:
        st.info("üí¨ B·∫Øt ƒë·∫ßu chat ƒë·ªÉ test c√¢u h·ªèi c·ªßa b·∫°n! Agent s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu th·ª±c.")
    else:
        chat_container = st.container()
        with chat_container:
            for msg in chat_history:
                # Safety check khi render
                content = msg["content"]
                if not isinstance(content, str):
                    content = str(content)
                render_chat_message(msg["role"], content, msg.get("timestamp"))
    
    # X·ª≠ l√Ω Quick Questions
    if quick_questions:
        st.markdown("**üí° C√¢u h·ªèi g·ª£i √Ω:**")
        cols = st.columns(min(len(quick_questions), 3))
        for i, q in enumerate(quick_questions[:6]):
            with cols[i % 3]:
                btn_label = q[:30] + "..." if len(q) > 30 else q
                if st.button(f"üí¨ {btn_label}", key=f"{context_key}_quick_{i}"):
                    # 1. Add User Question
                    add_to_chat_history(context_key, "user", q)
                    render_chat_message("user", q)
                    # 2. Stream & Accumulate
                    response_placeholder = st.empty()
                    gen = stream_agent_response(agent, q, response_placeholder)
                    
                    full_response = ""
                    logger.info(f"GENERATOR 1: {gen}")
                    for chunk in gen:
                        full_response += chunk
                        logger.info(f"CHUNK 1: {chunk}")
                    # 3. Save String Only
                    add_to_chat_history(context_key, "assistant", full_response)
                    logger.info(f"FULL RESPONSE 1: {full_response}")

                    st.rerun()
    
    st.markdown("---")
    with st.form(f"chat_form_{context_key}", clear_on_submit=True):
        user_input = st.text_input(
            "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:",
            placeholder=placeholder_text,
            key=f"chat_input_{context_key}"
        )
        
        col1, col2, col3 = st.columns([1, 1, 4])
        with col1:
            submitted = st.form_submit_button("üì§ G·ª≠i", type="primary")
        with col2:
            clear = st.form_submit_button("üóëÔ∏è X√≥a chat")
    
    if clear:
        clear_chat_history(context_key)
        st.rerun()
    
    if submitted and user_input:
        add_to_chat_history(context_key, "user", user_input)
        render_chat_message("user", user_input)
        response_placeholder = st.empty()
        
        # FIX: Loop generator to get full string
        gen = stream_agent_response(agent, user_input, response_placeholder)
        full_response = ""
        logger.info(f"GENERATOR 2: {gen}")

        for chunk in gen:
            full_response += chunk
            logger.info(f"CHUNK 2: {chunk}")
        add_to_chat_history(context_key, "assistant", full_response)
        logger.info(f"FULL RESPONSE 3: {full_response}")
        st.rerun()


def render_multi_table_chat_interface(
    session,
    sql_tool,
    context_key: str,
    placeholder_text: str = "H·ªèi v·ªÅ d·ªØ li·ªáu trong c√°c b·∫£ng...",
    quick_questions: List[str] = None
):
    """Render chat interface v·ªõi h·ªó tr·ª£ nhi·ªÅu b·∫£ng V√Ä STREAMING (Generator Safe)"""
    initialize_chat_state(context_key)
    
    tables_info = sql_tool.get_tables_info()
    with st.expander(f"üìä C√°c b·∫£ng c√≥ s·∫µn ({tables_info['total_tables']} b·∫£ng)", expanded=False):
        for table in tables_info["tables"]:
            cols_preview = ', '.join(str(c) for c in table['columns'][:6])
            if len(table['columns']) > 6:
                cols_preview += f" ... (+{len(table['columns']) - 6} c·ªôt)"
            st.markdown(f"‚Ä¢ **{table['name']}** ({table['row_count']} rows): {cols_preview}")
    
    chat_history = get_chat_history(context_key)
    
    if not chat_history:
        st.info("üí¨ B·∫Øt ƒë·∫ßu chat ƒë·ªÉ test c√¢u h·ªèi! Agent c√≥ th·ªÉ query tr√™n nhi·ªÅu b·∫£ng.")
    else:
        chat_container = st.container()
        with chat_container:
            for msg in chat_history:
                # Safety check
                content = msg["content"]
                if not isinstance(content, str):
                    content = str(content)
                render_chat_message(msg["role"], content, msg.get("timestamp"))
    
    # X·ª≠ l√Ω Quick Questions
    if quick_questions:
        st.markdown("**üí° C√¢u h·ªèi g·ª£i √Ω:**")
        cols = st.columns(min(len(quick_questions), 3))
        for i, q in enumerate(quick_questions[:6]):
            with cols[i % 3]:
                btn_label = q[:30] + "..." if len(q) > 30 else q
                if st.button(f"üí¨ {btn_label}", key=f"{context_key}_quick_{i}"):
                    # 1. Add User Question
                    add_to_chat_history(context_key, "user", q)
                    render_chat_message("user", q)
                    
                    # 2. Stream & Accumulate
                    response_placeholder = st.empty()
                    gen = _stream_multi_table_query(q, sql_tool, session, response_placeholder)
                    
                    full_response = ""
                    for chunk in gen:
                        full_response += chunk
                    
                    # 3. Save String Only
                    add_to_chat_history(context_key, "assistant", full_response)
                    logger.info(f"FULL RESPONSE 4: {full_response}")
                    st.rerun()
    
    st.markdown("---")
    with st.form(f"chat_form_{context_key}", clear_on_submit=True):
        user_input = st.text_input(
            "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:",
            placeholder=placeholder_text,
            key=f"chat_input_{context_key}"
        )
        
        col1, col2, col3 = st.columns([1, 1, 4])
        with col1:
            submitted = st.form_submit_button("üì§ G·ª≠i", type="primary")
        with col2:
            clear = st.form_submit_button("üóëÔ∏è X√≥a chat")
    
    if clear:
        clear_chat_history(context_key)
        st.rerun()
    
    if submitted and user_input:
        add_to_chat_history(context_key, "user", user_input)
        render_chat_message("user", user_input)
        
        response_placeholder = st.empty()
        
        # FIX: Loop generator to get full string
        gen = _stream_multi_table_query(user_input, sql_tool, session, response_placeholder)
        full_response = ""
        for chunk in gen:
            full_response += chunk
            
        add_to_chat_history(context_key, "assistant", full_response)
        logger.info(f"FULL RESPONSE 5: {full_response}")
        st.rerun()

# ============================================================================
# Debug Chat Interface Renderers
# ============================================================================

def render_chat_interface_debug(
    session,
    agent,
    context_key: str,
    placeholder_text: str = "H·ªèi m·ªôt c√¢u ƒë·ªÉ test...",
    quick_questions: List[str] = None
):
    """Render DEBUG chat interface v·ªõi streaming debug info"""
    initialize_chat_state(context_key)

    chat_history = get_chat_history(context_key)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠
    if not chat_history:
        st.info("üí¨ B·∫Øt ƒë·∫ßu chat ƒë·ªÉ test! Agent s·∫Ω hi·ªÉn th·ªã debug info.")
    else:
        chat_container = st.container()
        with chat_container:
            for msg in chat_history:
                content = msg["content"]
                if not isinstance(content, str):
                    content = str(content)
                render_chat_message(msg["role"], content, msg.get("timestamp"))

    # Quick Questions
    if quick_questions:
        st.markdown("**üí° C√¢u h·ªèi g·ª£i √Ω:**")
        cols = st.columns(min(len(quick_questions), 3))
        for i, q in enumerate(quick_questions[:6]):
            with cols[i % 3]:
                btn_label = q[:30] + "..." if len(q) > 30 else q
                if st.button(f"üí¨ {btn_label}", key=f"{context_key}_quick_debug_{i}"):
                    add_to_chat_history(context_key, "user", q)
                    render_chat_message("user", q)

                    response_placeholder = st.empty()
                    gen = stream_agent_response_debug(agent, q, response_placeholder)

                    full_response = ""
                    for chunk in gen:
                        full_response += chunk

                    add_to_chat_history(context_key, "assistant", full_response)
                    st.rerun()

    st.markdown("---")
    with st.form(f"chat_form_debug_{context_key}", clear_on_submit=True):
        user_input = st.text_input(
            "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:",
            placeholder=placeholder_text,
            key=f"chat_input_debug_{context_key}"
        )

        col1, col2, col3 = st.columns([1, 1, 4])
        with col1:
            submitted = st.form_submit_button("üì§ G·ª≠i", type="primary")
        with col2:
            clear = st.form_submit_button("üóëÔ∏è X√≥a chat")

    if clear:
        clear_chat_history(context_key)
        st.rerun()

    if submitted and user_input:
        add_to_chat_history(context_key, "user", user_input)
        render_chat_message("user", user_input)
        response_placeholder = st.empty()

        gen = stream_agent_response_debug(agent, user_input, response_placeholder)
        full_response = ""
        for chunk in gen:
            full_response += chunk

        add_to_chat_history(context_key, "assistant", full_response)
        st.rerun()


def render_multi_table_chat_interface_debug(
    session,
    sql_tool,
    context_key: str,
    placeholder_text: str = "H·ªèi v·ªÅ d·ªØ li·ªáu...",
    quick_questions: List[str] = None
):
    """Render DEBUG multi-table chat interface v·ªõi streaming debug info"""
    initialize_chat_state(context_key)

    tables_info = sql_tool.get_tables_info()
    with st.expander(f"üìä C√°c b·∫£ng c√≥ s·∫µn ({tables_info['total_tables']} b·∫£ng)", expanded=False):
        for table in tables_info["tables"]:
            cols_preview = ', '.join(str(c) for c in table['columns'][:6])
            if len(table['columns']) > 6:
                cols_preview += f" ... (+{len(table['columns']) - 6} c·ªôt)"
            st.markdown(f"‚Ä¢ **{table['name']}** ({table['row_count']} rows): {cols_preview}")

    chat_history = get_chat_history(context_key)

    if not chat_history:
        st.info("üí¨ B·∫Øt ƒë·∫ßu chat ƒë·ªÉ test! Agent s·∫Ω hi·ªÉn th·ªã debug info.")
    else:
        chat_container = st.container()
        with chat_container:
            for msg in chat_history:
                content = msg["content"]
                if not isinstance(content, str):
                    content = str(content)
                render_chat_message(msg["role"], content, msg.get("timestamp"))

    # Quick Questions
    if quick_questions:
        st.markdown("**üí° C√¢u h·ªèi g·ª£i √Ω:**")
        cols = st.columns(min(len(quick_questions), 3))
        for i, q in enumerate(quick_questions[:6]):
            with cols[i % 3]:
                btn_label = q[:30] + "..." if len(q) > 30 else q
                if st.button(f"üí¨ {btn_label}", key=f"{context_key}_quick_debug_{i}"):
                    add_to_chat_history(context_key, "user", q)
                    render_chat_message("user", q)

                    response_placeholder = st.empty()
                    gen = _stream_multi_table_query_debug(q, sql_tool, session, response_placeholder)

                    full_response = ""
                    for chunk in gen:
                        full_response += chunk

                    add_to_chat_history(context_key, "assistant", full_response)
                    st.rerun()

    st.markdown("---")
    with st.form(f"chat_form_debug_{context_key}", clear_on_submit=True):
        user_input = st.text_input(
            "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:",
            placeholder=placeholder_text,
            key=f"chat_input_debug_{context_key}"
        )

        col1, col2, col3 = st.columns([1, 1, 4])
        with col1:
            submitted = st.form_submit_button("üì§ G·ª≠i", type="primary")
        with col2:
            clear = st.form_submit_button("üóëÔ∏è X√≥a chat")

    if clear:
        clear_chat_history(context_key)
        st.rerun()

    if submitted and user_input:
        add_to_chat_history(context_key, "user", user_input)
        render_chat_message("user", user_input)

        response_placeholder = st.empty()

        gen = _stream_multi_table_query_debug(user_input, sql_tool, session, response_placeholder)
        full_response = ""
        for chunk in gen:
            full_response += chunk

        add_to_chat_history(context_key, "assistant", full_response)
        st.rerun()


# ============================================================================
# Add Chat Validation Functions
# ============================================================================

def add_chat_validation_to_questions_tab(session, df_cleaned, sql_tool,
                                          use_multi_table: bool = False,
                                          sources_dfs: Dict[str, pd.DataFrame] = None):
    """Add interactive chat validation to Questions tab - DEBUG MODE v·ªõi session ri√™ng"""
    st.divider()
    st.subheader("üîç Test C√¢u h·ªèi v·ªõi Agent (Debug Mode)")

    st.warning("‚ö†Ô∏è **Ch·∫ø ƒë·ªô Debug**: Hi·ªÉn th·ªã c√°ch LLM query ƒë·ªÉ ra c√¢u tr·∫£ l·ªùi. Session ri√™ng, kh√¥ng d√πng chung v·ªõi Agent Q&A tab.")

    if not session.question_set or not session.question_set.user_questions:
        st.info("üìù T·∫°o c√¢u h·ªèi ·ªü ph·∫ßn tr√™n, sau ƒë√≥ quay l·∫°i ƒë√¢y ƒë·ªÉ test!")
        return

    user_questions = [q.question for q in session.question_set.user_questions]

    # --- MULTI TABLE MODE (DEBUG) ---
    if use_multi_table and sources_dfs and len(sources_dfs) > 1:
        st.info(f"üìä **Ch·∫ø ƒë·ªô Multi-Table Debug**: Query tr√™n {len(sources_dfs)} b·∫£ng")

        # T·∫°o debug SQL tool v·ªõi session ID ri√™ng (th√™m _debug suffix)
        db_dir = Path("./agent_databases")
        db_dir.mkdir(exist_ok=True)
        debug_db_path = db_dir / f"debug_questions_{session.session_id}.db"

        debug_sql_tool = MultiTableSQLQueryTool(str(debug_db_path))
        for source_id, df in sources_dfs.items():
            debug_sql_tool.add_table(source_id, df)

        col1, col2 = st.columns([3, 1])
        with col1:
            st.success(f"üí° B·∫°n ƒë√£ t·∫°o **{len(user_questions)}** c√¢u h·ªèi. Test v·ªõi **{len(sources_dfs)} b·∫£ng** (Debug Mode)!")
        with col2:
            if st.button("üìã Copy c√¢u h·ªèi", key="copy_questions_mt_debug"):
                questions_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(user_questions)])
                st.code(questions_text)

        # S·ª≠ d·ª•ng debug chat interface
        render_multi_table_chat_interface_debug(
            session=session,
            sql_tool=debug_sql_tool,
            context_key="questions_validation_debug_multi",
            placeholder_text="V√≠ d·ª•: So s√°nh d·ªØ li·ªáu gi·ªØa c√°c b·∫£ng...",
            quick_questions=user_questions
        )

    # --- SINGLE TABLE MODE (DEBUG) ---
    else:
        # T·∫°o debug SQL tool v·ªõi session ID ri√™ng
        db_dir = Path("./agent_databases")
        db_dir.mkdir(exist_ok=True)
        debug_db_path = db_dir / f"debug_questions_single_{session.session_id}.db"

        debug_sql_tool = SQLQueryTool(str(debug_db_path), df_cleaned, "data")

        # T·∫°o debug agent ri√™ng (kh√¥ng d√πng chung v·ªõi validation_agent)
        if 'debug_questions_agent' not in st.session_state:
            from hst_agent import DataSchemaAgent
            debug_agent = DataSchemaAgent(
                session,
                st.session_state.api_key,
                st.session_state.model,
                df_cleaned=df_cleaned,
                sql_tool=debug_sql_tool
            )
            st.session_state.debug_questions_agent = debug_agent
        else:
            debug_agent = st.session_state.debug_questions_agent
            debug_agent.sql_tool = debug_sql_tool
            debug_agent.db_path = debug_sql_tool.db_path
            debug_agent.df_cleaned = df_cleaned

        col1, col2 = st.columns([3, 1])
        with col1:
            st.info(f"üí° B·∫°n ƒë√£ t·∫°o **{len(user_questions)}** c√¢u h·ªèi. Test v·ªõi agent (Debug Mode)!")
        with col2:
            if st.button("üìã Copy c√¢u h·ªèi", key="copy_questions_debug"):
                questions_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(user_questions)])
                st.code(questions_text)

        # S·ª≠ d·ª•ng debug chat interface
        render_chat_interface_debug(
            session=session,
            agent=debug_agent,
            context_key="questions_validation_debug",
            placeholder_text="V√≠ d·ª•: What is the average price?",
            quick_questions=user_questions
        )


def add_chat_validation_to_scenarios_tab(session, df_cleaned, sql_tool,
                                          use_multi_table: bool = False,
                                          sources_dfs: Dict[str, pd.DataFrame] = None):
    """Add interactive chat validation to Scenarios tab - DEBUG MODE v·ªõi session ri√™ng"""
    st.divider()
    st.subheader("üîç Test Scenario v·ªõi Agent (Debug Mode)")

    st.warning("‚ö†Ô∏è **Ch·∫ø ƒë·ªô Debug**: Hi·ªÉn th·ªã c√°ch LLM query ƒë·ªÉ ra c√¢u tr·∫£ l·ªùi. Session ri√™ng, kh√¥ng d√πng chung v·ªõi Agent Q&A tab.")

    if not session.scenarios:
        st.info("üìù T·∫°o scenario ·ªü ph·∫ßn tr√™n, sau ƒë√≥ quay l·∫°i ƒë√¢y ƒë·ªÉ test!")
        return

    scenario_names = [s.name for s in session.scenarios]
    selected_scenario_name = st.selectbox(
        "Ch·ªçn scenario ƒë·ªÉ test:",
        options=scenario_names,
        key="test_scenario_select_debug"
    )

    selected_scenario = next(
        (s for s in session.scenarios if s.name == selected_scenario_name),
        None
    )

    if not selected_scenario:
        return

    with st.expander("‚ÑπÔ∏è Th√¥ng tin Scenario", expanded=False):
        st.write(f"**T√™n:** {selected_scenario.name}")
        st.write(f"**M√¥ t·∫£:** {selected_scenario.description or 'N/A'}")
        st.write(f"**Selected Fields:** `{', '.join(selected_scenario.selected_fields)}`")
        st.write(f"**S·ªë c√¢u h·ªèi:** {len(selected_scenario.questions)}")

    is_multi = use_multi_table and sources_dfs and len(sources_dfs) > 1
    context_key = f"scenario_debug_{selected_scenario.id}_multi" if is_multi else f"scenario_debug_{selected_scenario.id}"

    # --- MULTI TABLE SCENARIO (DEBUG) ---
    if is_multi:
        st.info(f"üìä **Ch·∫ø ƒë·ªô Multi-Table Debug**: Query tr√™n {len(sources_dfs)} b·∫£ng")

        # T·∫°o debug SQL tool v·ªõi session ID ri√™ng
        db_dir = Path("./agent_databases")
        db_dir.mkdir(exist_ok=True)
        debug_db_path = db_dir / f"debug_scenario_{selected_scenario.id}_{session.session_id}.db"

        debug_sql_tool = MultiTableSQLQueryTool(str(debug_db_path))
        for source_id, df in sources_dfs.items():
            debug_sql_tool.add_table(source_id, df)

        # S·ª≠ d·ª•ng debug chat interface
        render_multi_table_chat_interface_debug(
            session=session,
            sql_tool=debug_sql_tool,
            context_key=context_key,
            placeholder_text="H·ªèi v·ªÅ scenario ho·∫∑c query tr√™n nhi·ªÅu b·∫£ng (Debug Mode)...",
            quick_questions=selected_scenario.questions
        )

        # === TEST ALL QUESTIONS (Multi-Table Debug) ===
        st.divider()
        st.markdown("### üß™ Test T·ª± ƒê·ªông (Debug Mode)")

        if st.button("‚ñ∂Ô∏è Test All Questions (Debug)", type="primary", key="auto_test_scenario_debug_multi"):
            clear_chat_history(context_key)

            for i, question in enumerate(selected_scenario.questions):
                st.markdown(f"---\n**[Q{i+1}] {question}**")
                add_to_chat_history(context_key, "user", f"[Q{i+1}] {question}")

                response_placeholder = st.empty()

                # G·ªçi debug generator
                gen = _stream_multi_table_query_debug(question, debug_sql_tool, session, response_placeholder)

                # Gom to√†n b·ªô text t·ª´ generator
                full_response = ""
                for chunk in gen:
                    full_response += chunk

                # L∆∞u text ƒë·∫ßy ƒë·ªß v√†o history
                add_to_chat_history(context_key, "assistant", full_response)

            st.success(f"‚úÖ ƒê√£ test {len(selected_scenario.questions)} c√¢u h·ªèi (Debug Mode)!")

    # --- SINGLE TABLE SCENARIO (DEBUG) ---
    else:
        # T·∫°o debug SQL tool v·ªõi session ID ri√™ng
        db_dir = Path("./agent_databases")
        db_dir.mkdir(exist_ok=True)
        debug_db_path = db_dir / f"debug_scenario_single_{selected_scenario.id}_{session.session_id}.db"

        debug_sql_tool = SQLQueryTool(str(debug_db_path), df_cleaned, "data")

        # T·∫°o debug agent ri√™ng cho scenario
        debug_agent_key = f'debug_scenario_agent_{selected_scenario.id}'
        if debug_agent_key not in st.session_state:
            from hst_agent import DataSchemaAgent
            debug_agent = DataSchemaAgent(
                session,
                st.session_state.api_key,
                st.session_state.model,
                df_cleaned=df_cleaned,
                sql_tool=debug_sql_tool
            )
            st.session_state[debug_agent_key] = debug_agent
        else:
            debug_agent = st.session_state[debug_agent_key]
            debug_agent.sql_tool = debug_sql_tool
            debug_agent.db_path = debug_sql_tool.db_path
            debug_agent.df_cleaned = df_cleaned

        # S·ª≠ d·ª•ng debug chat interface
        render_chat_interface_debug(
            session=session,
            agent=debug_agent,
            context_key=context_key,
            placeholder_text="H·ªèi m·ªôt c√¢u t·ª´ scenario (Debug Mode)...",
            quick_questions=selected_scenario.questions
        )

        # === TEST ALL QUESTIONS (Single Table Debug) ===
        st.divider()
        st.markdown("### üß™ Test T·ª± ƒê·ªông (Debug Mode)")

        if st.button("‚ñ∂Ô∏è Test All Questions (Debug)", type="primary", key="auto_test_scenario_debug"):
            clear_chat_history(context_key)

            for i, question in enumerate(selected_scenario.questions):
                st.markdown(f"---\n**[Q{i+1}] {question}**")
                add_to_chat_history(context_key, "user", f"[Q{i+1}] {question}")

                response_placeholder = st.empty()

                # G·ªçi debug generator
                gen = stream_agent_response_debug(debug_agent, question, response_placeholder)

                # Gom to√†n b·ªô text
                full_response = ""
                for chunk in gen:
                    full_response += chunk

                # L∆∞u text ƒë·∫ßy ƒë·ªß v√†o history
                add_to_chat_history(context_key, "assistant", full_response)

            st.success(f"‚úÖ ƒê√£ test {len(selected_scenario.questions)} c√¢u h·ªèi (Debug Mode)!")

# ============================================================================
# Export chat history
# ============================================================================

# ============================================================================
# FIXED: Export chat history (Safety Version)
# ============================================================================

def export_chat_history_to_json(context_key: str, filename: str = None):
    """Export chat history to JSON file with safety checks for generators"""
    chat_history = get_chat_history(context_key)
    
    if not chat_history:
        return None
    
    if filename is None:
        filename = f"chat_history_{context_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    # Sanitize history: Convert generators or non-serializable objects to string
    safe_history = []
    for msg in chat_history:
        safe_msg = msg.copy()
        content = safe_msg.get("content")
        
        # Ki·ªÉm tra n·∫øu content kh√¥ng ph·∫£i string (v√≠ d·ª• l√† generator), convert sang string
        if not isinstance(content, str):
            if content is None:
                safe_msg["content"] = ""
            else:
                # N·∫øu l√† generator, ta kh√¥ng c·ª©u ƒë∆∞·ª£c n·ªôi dung ƒë√£ m·∫•t, nh∆∞ng tr√°nh ƒë∆∞·ª£c crash
                safe_msg["content"] = str(content) 
        
        safe_history.append(safe_msg)
    
    export_data = {
        "context": context_key,
        "exported_at": datetime.now().isoformat(),
        "message_count": len(safe_history),
        "messages": safe_history
    }
    
    # D√πng default=str ƒë·ªÉ force convert m·ªçi th·ª© c√≤n s√≥t l·∫°i th√†nh string
    json_str = json.dumps(export_data, indent=2, ensure_ascii=False, default=str)
    return json_str

def render_export_chat_button(context_key: str):
    """Render button to export chat history"""
    chat_history = get_chat_history(context_key)
    
    if chat_history:
        json_str = export_chat_history_to_json(context_key)
        if json_str:
            st.download_button(
                label="‚¨áÔ∏è Export Chat History",
                data=json_str,
                file_name=f"chat_{context_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                key=f"export_chat_{context_key}"
            )


# ============================================================================
# Statistics and Analysis
# ============================================================================

def get_chat_statistics(context_key: str) -> Dict[str, Any]:
    """Get statistics about chat history (Safe version)"""
    chat_history = get_chat_history(context_key)
    
    if not chat_history:
        return {
            "total_messages": 0,
            "user_messages": 0,
            "agent_messages": 0,
            "avg_response_length": 0
        }
    
    user_msgs = [msg for msg in chat_history if msg["role"] == "user"]
    agent_msgs = [msg for msg in chat_history if msg["role"] == "assistant"]
    
    # T√≠nh t·ªïng ƒë·ªô d√†i an to√†n (ki·ªÉm tra xem content c√≥ ph·∫£i string kh√¥ng)
    total_length = 0
    for msg in agent_msgs:
        content = msg.get("content", "")
        if isinstance(content, str):
            total_length += len(content)
        # N·∫øu l√† generator ho·∫∑c object kh√°c, ta b·ªè qua ho·∫∑c t√≠nh l√† 0 ƒë·ªÉ kh√¥ng g√¢y l·ªói
    
    avg_length = total_length / len(agent_msgs) if agent_msgs else 0
    
    return {
        "total_messages": len(chat_history),
        "user_messages": len(user_msgs),
        "agent_messages": len(agent_msgs),
        "avg_response_length": avg_length
    }

def render_chat_statistics(context_key: str):
    """Render chat statistics"""
    stats = get_chat_statistics(context_key)
    
    if stats["total_messages"] > 0:
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("T·ªïng tin nh·∫Øn", stats["total_messages"])
        with col2:
            st.metric("C√¢u h·ªèi", stats["user_messages"])
        with col3:
            st.metric("C√¢u tr·∫£ l·ªùi", stats["agent_messages"])
        with col4:
            st.metric("ƒê·ªô d√†i TB", f"{stats['avg_response_length']:.0f} chars")